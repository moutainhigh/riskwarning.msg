# 进件数据定时调度cron表达式
dbscan.entry.cron=0 0/1 * * * *
dbscan.entry.appName=com.hncy58.spark2.dbscan.EntryCustomerDBSCANCluster
dbscan.entry.distance=3000
dbscan.entry.custCnt=5
dbscan.entry.secondsDiffToNow=8640000
dbscan.entry.eventCode=E001
dbscan.entry.online=1
dbscan.entry.outPath=/tmp/out/entry_dbscan

# 授信数据定时调度cron表达式
dbscan.credit.cron=30 0/1 * * * *
dbscan.credit.appName=com.hncy58.spark2.dbscan.CreditCustomerDBSCANCluster
dbscan.credit.distance=5000
dbscan.credit.custCnt=3
dbscan.credit.secondsDiffToNow=8640000
dbscan.credit.eventCode=E001
dbscan.credit.online=1
dbscan.credit.outPath=/tmp/out/credit_dbscan

# 数据查询URL
dbscan.impala.url=jdbc:impala://100.66.70.202:21050
# 结果数据导出文件目录前缀
dbscan.file.protocol.prefix=hdfs://hncy58

# Spark环境相关配置
hadoop.conf.dir=
yarn.conf.dir=
spark.home=
java.home=
spark.executor.user=hdfs

# Spark动态实例分配配置
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.initialExecutors=4
spark.dynamicAllocation.minExecutors=4
spark.dynamicAllocation.maxExecutors=20

# Spark执行相关配置
spark.driver.cores=2
spark.driver.memory=2g
spark.executor.cores=2
spark.executor.memory=1g
spark.num.executors=4
spark.cores.max=10
spark.main.res.path=file:/home/test/riskwarning/0524/lib/riskwarning.msg-0.0.1.jar.original
spark.master=yarn
spark.deploy.mode=cluster
spark.executer.jars=file:/home/test/riskwarning/0524/lib/scala-dbscan-0.0.1.jar,file:/home/test/riskwarning/0524/lib/kudu-spark2_2.11-1.6.0-cdh6.0.1.jar,file:/home/test/riskwarning/0524/lib/ImpalaJDBC41.jar
spark.executer.files=